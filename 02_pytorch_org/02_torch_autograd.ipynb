{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72f78ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e72aec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18, ResNet18_Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f633ec62",
   "metadata": {},
   "source": [
    "### torch.autograd\n",
    "is PyTorch's automatic differentiation engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5da541cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\chris/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59b066033abf40a3aa314f8b02a5a210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The example loads a pretrained resnet 18 model, creates random data tensor (image) with label.\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "data = torch.rand(1,3,64,64)\n",
    "labels = torch.rand(1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "373859cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "# creating a prediction also results in calculation of a gradient\n",
    "# for each of the model parameters at that data point\n",
    "prediction = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74b59fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the prediction and label to calculate loss.\n",
    "# Backprop applies the loss through the model.\n",
    "# Backprop uses the gradient calculated as part of the forward pass\n",
    "#\n",
    "# Autograd stores the gradients for each model parameter in the .grad attribute\n",
    "loss = (prediction - labels).sum() # individual differences, reduction\n",
    "loss.backward() # backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd13527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an optimizer (SGD).\n",
    "# register the parameters of the model in the optimizer\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54ad5382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .step() initiates gradient descent. The optimizer adjusts each parameter\n",
    "optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0625d687",
   "metadata": {},
   "source": [
    "# Exclusion from the DAG\n",
    "a directed acyclic graph is maintained with the computations performed. At each computation the gradient is maintained. Trafersing the graph in the forward direction (prediction) causes gradients to be saved. Traversing the graph in the reverse direction (backprop) allows the loss value to be applied to each parameter in the graph.\n",
    "\n",
    "The DAG is re-created (starting after each backward() call). This allows changing the structure of the graph during training - flow control statements allowed in the training algorithm.\n",
    "\n",
    "By default torch tracks all tensors that have requires_grad flag set to True. For tensors not requiring gradients setting this to False excludes it from the gradient computation DAG.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4737e899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does 'a' require gradients? : False\n",
      "Does 'b' require gradients? : True\n"
     ]
    }
   ],
   "source": [
    "# The output tensor of an operation will require gradients even if only a\n",
    "# single input gtensor has requires_grad=True\n",
    "x = torch.rand(5,5)\n",
    "y = torch.rand(5,5)\n",
    "z = torch.rand((5,5), requires_grad=True)\n",
    "\n",
    "a = x + y\n",
    "print(f\"Does 'a' require gradients? : {a.requires_grad}\")\n",
    "b = x + z\n",
    "print(f\"Does 'b' require gradients? : {b.requires_grad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66a19a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a NN parameters that don't compute gradients are \"frozen parameters\"\n",
    "# It is useful to freeze part of model if you know in advance you don't need those gradients\n",
    "# This can save computes and focus on the intended parameters\n",
    "#\n",
    "# Another case is fine tuning a pretrained network\n",
    "#\n",
    "# In fine tuning we freeze most of the model and only modify the layers\n",
    "# to make predictions on new labels.\n",
    "#\n",
    "# Here we load a pretrained resnet18 and freeze all the parameters...\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "# Freeze all the parmeters in the network\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd5332b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to fine tun just the last layer just replace that with a ne linear layer (unfrozen\n",
    "# by default).\n",
    "model.fc = nn.Linear(512, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6cba694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize (only the last layer)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a5d8362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the only parametersthat are computing gradients are the last stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf514a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
